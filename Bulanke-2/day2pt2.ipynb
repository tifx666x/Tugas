{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5ce122-3098-4e8c-acf0-8ed9f3b194ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dc995cf-e72c-4376-9c64-57306f9c3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size =0.2,random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc703c5a-ca83-49b5-9c86-75ae83fc96c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      2\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m30\u001b[39m,activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,input_shape \u001b[38;5;241m=\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),\n\u001b[0;32m      3\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m15\u001b[39m,activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      4\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m30\u001b[39m,activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m ])\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m,validation_data \u001b[38;5;241m=\u001b[39m (X_test,y_test))\n\u001b[0;32m      8\u001b[0m test_loss,test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test,y_test)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss pada data uji: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:694\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[1;34m(target, output, from_logits)\u001b[0m\n\u001b[0;32m    691\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    698\u001b[0m     )\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 30)"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(30,activation='relu',input_shape =(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(15,activation='relu'),\n",
    "    tf.keras.layers.Dense(30,activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, batch_size = 16,validation_data = (X_test,y_test))\n",
    "test_loss,test_acc = model.evaluate(X_test,y_test)\n",
    "print (f'Loss pada data uji: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "845020a5-8f90-4800-8e91-32bd3282ed28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data sebelum preeprocessing\n",
      "                                             reviews  label\n",
      "0  kemeja nya bagusss bgttttüòçüòçüòçaaaa mauuu nngisss...    1.0\n",
      "1  Jahitannya sih rapi,cuman ada benang yang ikut...    0.0\n",
      "2  Sesuai harga. Agak tipis tapi masih oke kok. W...    0.0\n",
      "3  Wah gila sihhh sebagus itu, se worth it, se  l...    1.0\n",
      "4  Kain nya bagus halus  \\nTapi kok di bukak koto...    0.0\n",
      "\n",
      "Sample Data Setelah  Preprocessing:\n",
      "0    kemeja nya bagusss bgttttaaaa mauuu nngisssssk...\n",
      "1    jahitannya sih rapicuman ada benang yang ikut ...\n",
      "2    sesuai harga agak tipis tapi masih oke kok war...\n",
      "3    wah gila sihhh sebagus itu se worth it se  lem...\n",
      "4    kain nya bagus halus  \\ntapi kok di bukak koto...\n",
      "Name: cleaned_review, dtype: object\n",
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "\n",
    "data = pd.read_csv(\"reviews.csv\")\n",
    "print(\"Sample data sebelum preeprocessing\")\n",
    "print(data.head())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]','',text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "data['cleaned_review']=data['reviews'].apply(preprocess_text)\n",
    "\n",
    "print (\"\\nSample Data Setelah  Preprocessing:\")\n",
    "print(data['cleaned_review'].head())\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['label'] = le.fit_transform(data['label'])\n",
    "print(data['label'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdc2e960-f890-49f6-ba6d-4158816e898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_review'],data['label'],\n",
    "                                                   test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq= tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = 100\n",
    "X_train_padded = pad_sequences(X_train_seq,maxlen=max_length, padding='post', truncating = 'post')\n",
    "X_test_padded = pad_sequences(X_test_seq,maxlen=max_length, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e322634d-0a2f-4d06-9b0a-cf66c6e9b9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.5099 - loss: 0.9685 - val_accuracy: 0.5030 - val_loss: 0.7428\n",
      "Epoch 2/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5394 - loss: 0.7154 - val_accuracy: 0.5030 - val_loss: 0.7198\n",
      "Epoch 3/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5454 - loss: 0.7070 - val_accuracy: 0.5030 - val_loss: 0.7015\n",
      "Epoch 4/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5476 - loss: 0.6990 - val_accuracy: 0.4970 - val_loss: 0.6985\n",
      "Epoch 5/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.5167 - loss: 0.6957 - val_accuracy: 0.5030 - val_loss: 0.7025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a20d749ca0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = 5000, output_dim=64, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(64, return_sequences= True),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(32,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer ='adam', metrics = ['accuracy'])\n",
    "model.fit(X_train_padded, y_train, epochs=5, validation_data=(X_test_padded,y_test), batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e8bf5e2-542d-4eba-8ac2-55860340e924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "Hasil Uji Sentimen:\n",
      "Teks: Film ini sangat bagus, saya menyukainya\n",
      "Sentimen: Positif\n"
     ]
    }
   ],
   "source": [
    "# 7. Uji Sentimen dengan Teks Baru\n",
    "new_text = [\"Film ini sangat bagus, saya menyukainya\"]\n",
    "new_text_cleaned = [preprocess_text(text) for text in new_text]\n",
    "new_text_seq = tokenizer.texts_to_sequences(new_text_cleaned)\n",
    "new_text_padded = pad_sequences(new_text_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Prediksi\n",
    "prediction = model.predict(new_text_padded)\n",
    "predicted_label = np.argmax(prediction, axis=1)  # Mengambil kelas dengan probabilitas tertinggi\n",
    "\n",
    "# Mengambil label dari encoded predictions\n",
    "sentiment_map = {0: 'Positif', 1: 'Netral', 2: 'Negatif'}\n",
    "predicted_sentiment = sentiment_map[predicted_label[0]]\n",
    "\n",
    "print(\"\\nHasil Uji Sentimen:\")\n",
    "print(f\"Teks: {new_text[0]}\")\n",
    "print(f\"Sentimen: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba0916-f59f-462e-ac01-191e77a97b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
